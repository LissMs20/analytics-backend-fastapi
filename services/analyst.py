import json
import pandas as pd
import numpy as np
import re
from typing import Dict, List, Any
from datetime import datetime

# Importa√ß√µes de schemas (Assumindo que schemas √© um m√≥dulo local ou est√° na raiz)
import schemas 

# Importa√ß√µes dos m√≥dulos modulares (mantidas as originais)
from .preprocessing import prepare_dataframe, extract_period_and_date
# Importa a fun√ß√£o de NLP para a L√≥gica 3
from .ia_core import classificar_observacao_topico 

AnalysisResponse = schemas.AnalysisResponse
Tip = schemas.Tip

# Assumindo que o ChartData √© o schema de Pydantic que voc√™ definiu para um √∫nico gr√°fico
ChartData = schemas.ChartData 

# --- FUN√á√ïES DE AN√ÅLISE DETALHADA (Auxiliares para Orquestra√ß√£o) ---

def _extract_origin_sector(causa_raiz: str) -> str:
    """Extrai o setor de origem da string de Causa Raiz de Processo."""
    if not isinstance(causa_raiz, str):
        return 'Setor Desconhecido'
    
    # Ex: 'Falha no Processo (M√°quina de Solda/Revis√£o)' -> 'M√°quina de Solda'
    match = re.search(r'\((.*?)\)', causa_raiz)
    if match:
        # Simplifica para o primeiro setor (ex: M√°quina de Solda)
        origin_str = match.group(1)
        return origin_str.split('/')[0].strip()
    return 'Geral/Outros'

def run_quality_analysis(df: pd.DataFrame, query: str) -> Dict[str, Any]:
    """L√ìGICA 1: Calcula e visualiza o DPPM (Defeitos por Milh√£o) ao longo do tempo."""
    
    period, specific_date, granularity_name = extract_period_and_date(query)
    df_filtered = df.copy()
    
    # 1. Aplica o filtro de data (Se necess√°rio, aprimorar a l√≥gica de filtragem de data relativa aqui)
    if specific_date:
        if period == 'D' or ('day' in str(specific_date) and period == 'G'): 
            df_filtered = df_filtered[df_filtered['data_registro'].dt.date == specific_date.date()]
            granularity_name = "Di√°ria"
            period = 'D'
        elif period == 'M' or ('day' not in str(specific_date)):
            df_filtered = df_filtered[
                (df_filtered['data_registro'].dt.year == specific_date.year) & 
                (df_filtered['data_registro'].dt.month == specific_date.month)
            ]
            granularity_name = f"Mensal ({specific_date.strftime('%m/%Y')})"
            period = 'D' 

    if df_filtered.empty:
        return {'status': 'FAIL', 'summary': f"N√£o h√° dados para o per√≠odo solicitado: **{granularity_name}**.", 'visualization_data': []}

    # 2. Agrega√ß√£o e C√°lculo de DPPM (Melhoria 1)
    if period == 'G':
        period = 'M' 
        granularity_name = 'Mensal'

    # Reseta o √≠ndice de falhas individuais para a contagem correta
    df_filtered = df_filtered.reset_index(drop=True) 

    df_filtered['periodo'] = df_filtered['data_registro'].dt.to_period(period).astype(str)
    
    summary_data = df_filtered.groupby('periodo').agg(
        total_falhas_periodo=('quantidade', 'sum'),
        total_producao_periodo=('quantidade_produzida', 'sum') 
    ).reset_index()

    summary_data['total_producao_safe'] = summary_data['total_producao_periodo'].apply(lambda x: x if x > 0 else 1)
    # C√°lculo do DPPM (Defeitos por Milh√£o)
    summary_data['dppm_periodo'] = (summary_data['total_falhas_periodo'] / summary_data['total_producao_safe']) * 1_000_000

    # 3. Resumo
    media_geral = summary_data['dppm_periodo'].mean()
    top_period_dppm = summary_data.sort_values(by='dppm_periodo', ascending=False).iloc[0] if not summary_data.empty else {'periodo': 'N/A', 'dppm_periodo': 0}

    resumo = f"""
        **An√°lise de Qualidade ({granularity_name})**
        A **m√©dia de DPPM** no per√≠odo analisado √© de **{media_geral:.2f}**.
        O per√≠odo com o **maior DPPM** foi **{top_period_dppm['periodo']}**, com **{top_period_dppm['dppm_periodo']:.2f}**.
        O controle de qualidade deve focar em reduzir o DPPM m√©dio e analisar o per√≠odo de pico.
    """
    
    # ATEN√á√ÉO: vis_data (um √∫nico gr√°fico) deve ser retornado como uma lista de um item.
    vis_data = ChartData(
        title=f"Tend√™ncia do DPPM (Defeitos por Milh√£o) - Agrega√ß√£o {granularity_name}",
        labels=summary_data['periodo'].tolist(),
        datasets=[
            {"label": "DPPM", "data": summary_data['dppm_periodo'].tolist(), "type": 'line', "borderColor": 'rgb(255, 99, 132)', "backgroundColor": 'rgba(255, 99, 132, 0.5)'}
        ],
        chart_type='line' # Adicionado o tipo de gr√°fico
    )
    
    dicas = [
        Tip(title="Foco no Desvio", detail=f"Analise o per√≠odo de pico ({top_period_dppm['periodo']}) para identificar a causa do alto DPPM."),
        Tip(title="Meta", detail=f"O acompanhamento √© crucial. Tente reduzir a m√©dia geral para **{(media_geral * 0.9):.2f}** DPPM."),
        Tip(title="M√©trica", detail="DPPM √© a m√©trica padr√£o da ind√∫stria para defeitos de qualidade de produ√ß√£o."),
    ]
    
    return {'status': 'OK', 'summary': resumo, 'visualization_data': [vis_data.model_dump()], 'tips': dicas}

def run_root_cause_analysis(df: pd.DataFrame, query: str) -> Dict[str, Any]:
    """L√ìGICA 2: Analisa a distribui√ß√£o de falhas por Causa Raiz e Linha de Produto."""
    
    causa_raiz_counts = df['causa_raiz_processo'].value_counts(normalize=True).mul(100).round(2).reset_index(name='percentual')
    causa_raiz_counts.columns = ['causa_raiz', 'percentual']
    
    top_causa = causa_raiz_counts.iloc[0]['causa_raiz'] if not causa_raiz_counts.empty else "N/A"
    top_causa_perc = causa_raiz_counts.iloc[0]['percentual'] if not causa_raiz_counts.empty else 0.0

    linha_counts = df['linha_produto'].value_counts(normalize=True).mul(100).round(2).reset_index(name='percentual')
    linha_counts.columns = ['linha_produto', 'percentual']
    top_linha = linha_counts.iloc[0]['linha_produto'] if not linha_counts.empty else "N/A"
    top_linha_perc = linha_counts.iloc[0]['percentual'] if not linha_counts.empty else 0.0

    resumo = f"""
        **An√°lise de Prioridade (Conhecimento de Processo)**
        A principal causa de falhas √© **{top_causa}**, representando **{top_causa_perc}%** do total.
        A linha de produtos com maior incid√™ncia de falhas √© a **Linha {top_linha}** (**{top_linha_perc}%** das ocorr√™ncias).
    """
    
    # ATEN√á√ÉO: vis_data (um √∫nico gr√°fico) deve ser retornado como uma lista de um item.
    vis_data = ChartData(
        title="Distribui√ß√£o de Falhas por Causa Raiz (Processo)",
        labels=causa_raiz_counts['causa_raiz'].tolist(),
        datasets=[
            {"label": "Percentual de Falhas", "data": causa_raiz_counts['percentual'].tolist(), "type": 'bar'}
        ],
        chart_type='bar'
    )
    
    dicas = [
        Tip(title=f"A√ß√£o Priorit√°ria (Causa)", detail=f"Concentre esfor√ßos na causa **'{top_causa}'**."),
        Tip(title=f"A√ß√£o Priorit√°ria (Produto)", detail="Realize auditorias nos procedimentos de montagem e teste dos produtos da Linha de " + top_linha + "."),
    ]
    
    return {'status': 'OK', 'summary': resumo, 'visualization_data': [vis_data.model_dump()], 'tips': dicas}

def run_nlp_analysis(df: pd.DataFrame, query: str) -> Dict[str, Any]:
    """L√ìGICA 3: Analisa a distribui√ß√£o de falhas por T√≥pico (NLP do texto livre)."""
    
    if df['observacao_combinada'].isnull().all() or df['observacao_combinada'].str.strip().eq('').all():
        return {'status': 'FAIL', 'summary': "An√°lise de T√≥picos n√£o executada: A maioria das observa√ß√µes est√° vazia ou nula.", 'visualization_data': []}
    
    # Aplica a fun√ß√£o de NLP do ia_core
    df['causa_raiz_ia'] = df['observacao_combinada'].apply(classificar_observacao_topico)
    
    analise_topico = df.groupby('causa_raiz_ia')['documento_id'].count().sort_values(ascending=False).reset_index(name='Contagem')
    
    if analise_topico.empty or analise_topico.iloc[0]['causa_raiz_ia'].startswith("N/A"):
        return {'status': 'FAIL', 'summary': "A an√°lise de observa√ß√µes n√£o retornou resultados v√°lidos (modelo NLP indispon√≠vel ou textos muito curtos).", 'visualization_data': []}
            
    # ATEN√á√ÉO: vis_data (um √∫nico gr√°fico) deve ser retornado como uma lista de um item.
    vis_data = ChartData(
        title="Contagem por Causa Raiz (An√°lise IA do Texto)",
        labels=analise_topico['causa_raiz_ia'].tolist(),
        datasets=[
            {"label": "Contagem", "data": analise_topico['Contagem'].tolist(), "backgroundColor": ["#4bc0c0", "#ff6384", "#ffcd56", "#36a2eb", "#9966ff"]}
        ],
        chart_type='pie' # Sugerido Pie para distribui√ß√£o de t√≥picos
    )
    
    top_causa_ia = analise_topico.iloc[0]['causa_raiz_ia']
    
    resumo = f"A principal causa raiz identificada (via texto livre - NLP) √© **{top_causa_ia}**, com {analise_topico.iloc[0]['Contagem']} ocorr√™ncias."
    
    dicas = [
        Tip(title="A√ß√£o por T√≥pico", detail=f"Se a principal causa √© '{top_causa_ia}', revise as instru√ß√µes ou materiais para a preven√ß√£o."),
        Tip(title="Valida√ß√£o", detail="Compare esta classifica√ß√£o de NLP com a classifica√ß√£o tabular para validar a precis√£o.")
    ]
    
    return {'status': 'OK', 'summary': resumo, 'visualization_data': [vis_data.model_dump()], 'tips': dicas}

def run_sector_analysis(df: pd.DataFrame, query: str) -> Dict[str, Any]:
    """L√ìGICA 4: Analisa o setor de DETEC√á√ÉO e o setor de ORIGEM, retornando DOIS gr√°ficos."""
    
    if df.empty:
        return {'status': 'FAIL', 'summary': "An√°lise de Setor n√£o executada: DataFrame vazio.", 'visualization_data': []}

    # --- 1. An√°lise de Setor de DETEC√á√ÉO (Onde a falha foi encontrada) ---
    falhas_por_setor_deteccao = df.groupby('setor_falha_individual')['falha_individual'] \
                            .count() \
                            .sort_values(ascending=False) \
                            .reset_index(name='Contagem')
    
    if falhas_por_setor_deteccao.empty:
        return {'status': 'FAIL', 'summary': "An√°lise de Setor n√£o executada: Dados de setor de falha ausentes.", 'visualization_data': []}
            
    top_setor_deteccao = falhas_por_setor_deteccao.iloc[0]['setor_falha_individual']

    # Gr√°fico 1: Setor de Detec√ß√£o
    chart_deteccao = ChartData(
        title="1. Volume de Falhas por Setor de DETEC√á√ÉO",
        labels=falhas_por_setor_deteccao['setor_falha_individual'].tolist(),
        datasets=[
            {"label": "Total de Falhas", "data": falhas_por_setor_deteccao['Contagem'].tolist(), "backgroundColor": "rgba(255, 99, 132, 0.7)" }
        ],
        chart_type='bar'
    )

    # --- 2. An√°lise de Setor de ORIGEM (Onde o problema foi causado) ---
    df['setor_origem'] = df['causa_raiz_processo'].apply(_extract_origin_sector)
    falhas_por_setor_origem = df.groupby('setor_origem')['documento_id'].count().sort_values(ascending=False).reset_index(name='Contagem')
    
    top_origem = falhas_por_setor_origem.iloc[0]['setor_origem'] if not falhas_por_setor_origem.empty else "N/A"

    # Gr√°fico 2: Setor de Origem
    chart_origem = ChartData(
        title="2. Volume de Falhas por Setor de ORIGEM",
        labels=falhas_por_setor_origem['setor_origem'].tolist(),
        datasets=[
            {"label": "Total de Falhas", "data": falhas_por_setor_origem['Contagem'].tolist(), "backgroundColor": "rgba(54, 162, 235, 0.7)" }
        ],
        chart_type='bar'
    )
    
    # üí° NOVO: An√°lise detalhada das falhas no setor de origem principal
    falhas_do_top_origem = df[df['setor_origem'] == top_origem]
    top_falhas_no_origem = falhas_do_top_origem['falha_individual'].value_counts().head(3).reset_index()
    top_falhas_no_origem.columns = ['falha', 'contagem']

    falhas_detalhadas = ""
    if not top_falhas_no_origem.empty:
        falhas_detalhadas = "As falhas mais comuns neste setor de origem s√£o:\n"
        for _, row in top_falhas_no_origem.iterrows():
            falhas_detalhadas += f"- **{row['falha']}** ({row['contagem']} ocorr√™ncias)\n"
    
    # --- 3. Resumo Combinado e Melhorado ---
    resumo = f"""
        **An√°lise de Setor (Detec√ß√£o vs. Origem)**
        
        O setor de **DETEC√á√ÉO** com o maior volume de falhas √© **{top_setor_deteccao}** ({falhas_por_setor_deteccao.iloc[0]['Contagem']} ocorr√™ncias).
        
        No entanto, o setor de **ORIGEM** mais prov√°vel das falhas √© **{top_origem}**. Isso indica que a maioria dos problemas est√° sendo criada l√°.
        
        {falhas_detalhadas}
        
        **A√ß√£o Recomendada:** Concentre a investiga√ß√£o de **Causa Raiz** no setor de **ORIGEM** ({top_origem}) para corrigir os processos que geram as falhas listadas.
    """
    
    dicas = [
        Tip(title="A√ß√£o Estrat√©gica", detail=f"O foco deve ser a melhoria cont√≠nua dos processos do setor de **ORIGEM** ({top_origem})."),
        Tip(title="Ponto de Controle", detail=f"O setor de **DETEC√á√ÉO** ({top_setor_deteccao}) deve ser mantido como o principal ponto de controle de qualidade."),
    ]
    
    # 4. Retorna uma LISTA de gr√°ficos, convertendo os Pydantic models para dicion√°rios
    return {
        'status': 'OK', 
        'summary': resumo, 
        'visualization_data': [chart_deteccao.model_dump(), chart_origem.model_dump()], 
        'tips': dicas
    }

def default_analysis(df: pd.DataFrame, query: str) -> AnalysisResponse:
    """An√°lise de fallback se nenhuma query espec√≠fica for encontrada."""
    try:
        falha_col = 'falha_individual' if 'falha_individual' in df.columns else 'falha'
        top_falha = df[falha_col].dropna().mode().iat[0]
    except Exception:
        top_falha = "N/A (Coluna de falha vazia)"
        
    return AnalysisResponse(
        query=query,
        summary=f"A IA realizou uma an√°lise geral sobre **{len(df)}** registros. A falha mais comum √© **'{top_falha}'**. Nenhuma consulta espec√≠fica foi detectada.",
        tips=[
            Tip(title="Sugest√£o de Busca Avan√ßada", detail="Tente buscar por **'taxa de falha di√°ria'**, **'causa raiz'** ou **'t√≥pico das observa√ß√µes'**.")
        ],
        visualization_data=[] # Garante que o fallback retorne uma lista vazia
    )

def run_dppm_definition() -> Dict[str, Any]:
    """L√ìGICA DEFINITION: Explica o que √© DPPM."""
    
    summary = """
        **DPPM** significa **Defeitos Por Milh√£o**.
        
        √â uma m√©trica de qualidade que mede a quantidade de pe√ßas defeituosas ou falhas encontradas a cada 1 milh√£o de unidades produzidas.
        
        **Por que √© importante?**
        - **Padroniza√ß√£o:** Permite comparar a qualidade de diferentes processos ou f√°bricas.
        - **Alta Precis√£o:** √â ideal para processos de alta qualidade, onde a taxa de defeitos √© muito baixa (ex: 0,05%).
        - **An√°lise de Tend√™ncia:** Seu acompanhamento ao longo do tempo indica se o processo de produ√ß√£o est√° melhorando (DPPM diminuindo) ou piorando (DPPM aumentando).
    """
    
    dicas = [
        Tip(title="C√°lculo R√°pido", detail="DPPM = (Total de Defeitos / Total Produzido) * 1.000.000"),
        Tip(title="Meta 6 Sigma", detail="O objetivo final da metodologia 6 Sigma √© atingir um DPPM de apenas 3,4."),
    ]
    
    return {
        'status': 'OK',
        'summary': summary,
        'visualization_data': [],
        'tips': dicas
    }

# NOVA FUN√á√ÉO: Resposta a cumprimentos simples
def run_greeting_analysis(query: str) -> Dict[str, Any]:
    """L√ìGICA GREETING: Responde a cumprimentos e oferece sugest√µes de consulta."""
    
    # Determina o cumprimento baseado na hora do dia (melhora a personaliza√ß√£o)
    current_hour = datetime.now().hour
    if 5 <= current_hour < 12:
        greeting = "Bom dia!"
    elif 12 <= current_hour < 18:
        greeting = "Boa tarde!"
    else:
        greeting = "Boa noite!"

    summary = (
        f"{greeting} Eu sou a **Cortex**, a intelig√™ncia artificial do Tron Analytics. Em que posso te ajudar hoje?"
        "\n\nPara come√ßar, voc√™ pode me perguntar sobre:"
    )

    # Dicas de consulta n√£o √≥bvias
    dicas = [
        Tip(title="Tend√™ncia de Qualidade", detail="Pergunte: 'Qual √© o DPPM mensal da produ√ß√£o?'"),
        Tip(title="Foco Geogr√°fico", detail="Pergunte: 'Quais s√£o as falhas mais comuns no lado A da placa?'"),
        Tip(title="Desvio de Processo", detail="Pergunte: 'A principal causa raiz tem rela√ß√£o com o setor de SMT?'"),
        Tip(title="An√°lise Preditiva", detail="Pergunte: 'Existe alguma observa√ß√£o que indique problemas com o processo de solda?'"),
    ]
    
    # Retorna uma resposta estruturada sem dados de visualiza√ß√£o
    return {
        'status': 'OK', 
        'summary': summary, 
        'visualization_data': [], # Lista vazia, pois n√£o h√° gr√°fico
        'tips': dicas
    }

# --- FUN√á√ÉO PRINCIPAL DE ORQUESTRA√á√ÉO DE AN√ÅLISE ---

def handle_query_analysis(query: str, data_to_analyze: List[Dict]) -> AnalysisResponse:
    """
    Fun√ß√£o principal de IA que orquestra as an√°lises estat√≠sticas/dom√≠nio
    com base na consulta do usu√°rio.
    """
    query_lower = query.lower()
    analysis_results = {}
    
    # 0. NOVO: TRATAMENTO DE CUMPRIMENTOS (Prioridade M√°xima)
    # Define padr√µes que indicam apenas um cumprimento simples
    greeting_patterns = r'^(oi|ol√°|bom dia|boa tarde|boa noite|tudo bem|e a√≠)[\s.,!?]*$'
    if re.match(greeting_patterns, query_lower):
        return AnalysisResponse(
            query=query,
            **run_greeting_analysis(query)
        )

    # NOVO: TRATAMENTO DE DEFINI√á√ïES (Prioridade Alta)
    definition_patterns = [
        'o que √© dppm', 'dppm o que √©', 'o que significa dppm', 
        'defini√ß√£o de dppm', 'o que e dppm', 

        'o que √© ddpm', 'ddpm o que √©', 'o que significa ddpm', 
    ]
    if any(pattern in query_lower for pattern in definition_patterns):
        return AnalysisResponse(
            query=query,
            **run_dppm_definition()
        )

    # 1. Pr√©-processamento e Flatten Cr√≠tico
    df = prepare_dataframe(data_to_analyze, flatten_multifalha=True) 

    if df.empty or len(data_to_analyze) == 0:
        # Resposta de fallback para base de dados vazia
        return AnalysisResponse(
            query=query,
            summary="Nenhum dado encontrado para an√°lise ou dados inv√°lidos ap√≥s o pr√©-processamento. Tente consultar dados em um per√≠odo diferente.",
            tips=[Tip(title="Base de Dados Vazia", detail="Verifique a fonte de dados e o filtro inicial.")],
            visualization_data=[]
        )

    # 2. Mapeamento de Fun√ß√µes de An√°lise e Execu√ß√£o (mantido o resto da l√≥gica)
    
    # Tenta executar a an√°lise mais espec√≠fica correspondente √† query
    if 'taxa de falha' in query_lower or 'dppm' in query_lower or 'qualidade' in query_lower:
        analysis_results = run_quality_analysis(df, query)
    elif 'causa raiz' in query_lower or 'linha' in query_lower:
        analysis_results = run_root_cause_analysis(df, query)
    elif 't√≥pico' in query_lower or 'nlp' in query_lower or 'observa√ß√µes' in query_lower:
        analysis_results = run_nlp_analysis(df, query)
    elif 'setor' in query_lower or 'origem' in query_lower:
        analysis_results = run_sector_analysis(df, query)
    
    # 3. Consolida√ß√£o dos Resultados
    if analysis_results and analysis_results.get('status') == 'OK':
        return AnalysisResponse(
            query=query,
            summary=analysis_results['summary'],
            tips=analysis_results['tips'],
            visualization_data=analysis_results['visualization_data']
        )
    
    # 4. Fallback para An√°lise Geral
    return default_analysis(df, query)