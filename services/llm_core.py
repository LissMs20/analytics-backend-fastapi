# services/llm_core.py

import os
import pandas as pd
import json
from typing import Dict, Any, List
from google import genai
from google.genai import types
from google.genai.errors import APIError
from starlette.concurrency import run_in_threadpool 

# --- CONFIGURA√á√ÉO DO CLIENTE GEMINI ---
client = None
GEMINI_MODEL = "gemini-2.5-flash" # Modelo r√°pido para classifica√ß√£o e sumariza√ß√£o

try:
    # A vari√°vel de ambiente GEMINI_API_KEY deve ser configurada no ambiente (Render)
    client = genai.Client()
    print("INFO: Cliente Gemini inicializado com sucesso.")
except Exception as e:
    # Este log √© crucial para identificar falha na chave de API
    print(f"ERRO CR√çTICO: Cliente Gemini N√ÉO inicializado. Verifique a API KEY. Detalhe: {e}")

# --- FUN√á√ïES AUXILIARES DE FORMATA√á√ÉO ---
def format_data_for_llm(df: pd.DataFrame) -> str:
    """
    Formata as observa√ß√µes combinadas em uma string simples para o LLM.
    Reduzir este limite √© a MELHOR forma de evitar timeout.
    """
    context_df = df[['documento_id', 'observacao_combinada']].copy()
    # üö® OTIMIZA√á√ÉO: Limite de 100 registros para evitar timeout.
    context_df = context_df.dropna(subset=['observacao_combinada']).head(100)

    if context_df.empty:
        return "Nenhuma observa√ß√£o de texto livre v√°lida foi encontrada no conjunto de dados para an√°lise."
    
    formatted_str = "Lista de Observa√ß√µes de Falhas (ID: Texto):\n"
    for index, row in context_df.iterrows():
        # Limita o texto de cada observa√ß√£o (200 chars) para garantir que n√£o haja estouro de token
        text_content = row['observacao_combinada'].replace('\n', ' ').strip()
        formatted_str += f"{row['documento_id']}: \"{text_content[:200]}\"\n" 
        
    return formatted_str

# --- Fun√ß√µes Auxiliares S√≠ncronas para o Threadpool ---
def _generate_content_sync(model: str, contents: str, config: types.GenerateContentConfig = None):
    """Fun√ß√£o s√≠ncrona que chama a API Gemini. Rodar√° em um threadpool."""
    if client is None:
        raise Exception('Cliente Gemini n√£o est√° inicializado para chamada s√≠ncrona.')
        
    return client.models.generate_content(
        model=model,
        contents=contents,
        config=config,
    )
# --------------------------------------------------------

# --- 1. CLASSIFICA√á√ÉO DE INTEN√á√ÉO (Inten√ß√£o do Usu√°rio) ---

INTENT_SCHEMA = {
    "type": "array",
    "description": "Lista de inten√ß√µes detectadas na consulta do usu√°rio.",
    "items": {
        "type": "string",
        "enum": ["qualidade", "setor", "causa_raiz", "nlp", "default"]
    }
}

async def classify_query_intent(query: str) -> List[str]:
    """
    Classifica a inten√ß√£o da consulta do usu√°rio usando Gemini, retornando uma lista JSON estruturada.
    """
    if client is None:
        print("AVISO: classify_query_intent - Cliente Gemini n√£o inicializado. Usando default.")
        # Retorna default para evitar que a falha do cliente quebre o fluxo
        return ["default"] 

    prompt = f"""
    Sua tarefa √© classificar a inten√ß√£o da seguinte consulta do usu√°rio, usando APENAS as categorias pr√©-definidas.
    
    Categorias:
    - qualidade: Se a consulta focar em m√©tricas (dppm, rejei√ß√£o, falha, taxa, tend√™ncia, percentual).
    - setor: Se a consulta focar em √°reas, departamentos, localiza√ß√£o de origem ou detec√ß√£o.
    - causa_raiz: Se a consulta focar em processos, linhas de produto, produtos espec√≠ficos (placas, componentes), ou a raiz do problema.
    - nlp: Se a consulta focar em an√°lise de texto, observa√ß√µes, coment√°rios ou t√≥picos de texto livre.
    - default: Se a consulta n√£o se encaixar claramente em nenhuma das outras categorias (ex: sauda√ß√µes, ou pedidos gen√©ricos).

    Retorne APENAS um array JSON contendo UMA OU MAIS categorias que se aplicam.

    Consulta do Usu√°rio: "{query}"
    """
    
    config = types.GenerateContentConfig(
        response_mime_type="application/json",
        response_schema=INTENT_SCHEMA,
    )

    try:
        response = await run_in_threadpool(
            _generate_content_sync,
            GEMINI_MODEL,
            prompt,
            config
        )
        
        # O modelo deve retornar um JSON v√°lido (array de strings)
        return json.loads(response.text)
        
    except (APIError, json.JSONDecodeError) as e:
        # Erro de API ou de parsing do JSON
        print(f"ERRO DE CLASSIFICA√á√ÉO: Falha na inten√ß√£o. Detalhe: {e}") 
        return ["default"]
    except Exception as e:
        # Erro gen√©rico (ex: falha do run_in_threadpool ao chamar _generate_content_sync)
        print(f"ERRO GEN√âRICO CLASSIFICA√á√ÉO: Detalhe: {e}")
        return ["default"]


# --- 2. AN√ÅLISE DE OBSERVA√á√ïES (NLP) ---

TOPIC_ANALYSIS_SCHEMA = {
    "type": "object",
    "properties": {
        "resumo_ia": {"type": "string", "description": "Um resumo de 2 a 3 frases dos principais achados e tend√™ncias no texto."},
        "topicos_ia": {
            "type": "array",
            "description": "Lista dos 5 t√≥picos mais comuns extra√≠dos do texto, com contagem associada.",
            "items": {
                "type": "object",
                "properties": {
                    "nome": {"type": "string", "description": "Nome do t√≥pico ou causa raiz principal."},
                    "contagem": {"type": "integer", "description": "Contagem de ocorr√™ncias deste t√≥pico na lista de observa√ß√µes."}
                },
                "required": ["nome", "contagem"]
            }
        }
    },
    "required": ["resumo_ia", "topicos_ia"]
}


async def analyze_observations_with_gemini(df: pd.DataFrame, query: str) -> Dict[str, Any]:
    """
    Chama a API Gemini para classificar e resumir os t√≥picos das observa√ß√µes.
    """
    if client is None:
        print("ERRO CR√çTICO: analyze_observations_with_gemini - Cliente Gemini n√£o inicializado.")
        return {'status': 'ERROR', 'error': 'Cliente Gemini n√£o inicializado. Verifique a configura√ß√£o da API KEY.'}

    observations_context = format_data_for_llm(df)

    if observations_context.startswith("Nenhuma"):
        return {'status': 'FAIL', 'error': observations_context}

    prompt_instruction = f"""
    Voc√™ √© um especialista em An√°lise de Causa Raiz de Manufatura.
    Com base na lista de observa√ß√µes abaixo, realize uma An√°lise de T√≥picos (NLP) para identificar as 5 principais causas raiz ou temas que est√£o sendo relatados nos coment√°rios.

    1. GERE uma an√°lise concisa (resumo_ia) de 2 a 3 frases.
    2. CLASSIFIQUE os 5 t√≥picos mais relevantes (topicos_ia) e conte quantas vezes cada t√≥pico ou sin√¥nimo √© mencionado (contagem).

    Dados de Observa√ß√£o:
    ---
    {observations_context}
    ---

    Consulta do Usu√°rio (Para Contexto): {query}

    Retorne o resultado estritamente no formato JSON definido.
    """
    
    config = types.GenerateContentConfig(
        response_mime_type="application/json",
        response_schema=TOPIC_ANALYSIS_SCHEMA
    )

    try:
        # PONTO CHAVE: Usando run_in_threadpool
        response = await run_in_threadpool(
            _generate_content_sync,
            GEMINI_MODEL,
            prompt_instruction,
            config
        )

        if not response.text:
            print("ERRO NLP: Resposta vazia do modelo Gemini.")
            return {'status': 'FAIL', 'error': 'Resposta vazia do modelo Gemini.'}
            
        llm_analysis_data = json.loads(response.text)
        
        return {
            'status': 'OK',
            'summary': llm_analysis_data.get('resumo_ia', 'Resumo n√£o gerado pelo modelo.'),
            'topics_data': llm_analysis_data.get('topicos_ia', [])
        }
    
    except APIError as e:
        print(f"ERRO API (NLP): Falha na an√°lise de observa√ß√µes. Detalhe: {e}")
        return {'status': 'ERROR', 'error': f'Erro na API Gemini: {e}'}
    except json.JSONDecodeError:
        # Tenta pegar a resposta bruta para debug, se poss√≠vel
        raw_text = response.text if 'response' in locals() and hasattr(response, 'text') else "N/A"
        print(f"ERRO JSON (NLP): Modelo n√£o retornou JSON v√°lido. Detalhe: {raw_text[:100]}...")
        return {'status': 'ERROR', 'error': f'O modelo Gemini n√£o retornou JSON v√°lido. Resposta bruta: {raw_text[:100]}...'}
    except Exception as e:
        print(f"ERRO GEN√âRICO (NLP): Erro desconhecido: {e}")
        return {'status': 'ERROR', 'error': f'Erro desconhecido: {e}'}

# --- 3. SUMARIZA√á√ÉO ESTRAT√âGICA (Insight Executivo) ---

async def summarize_analysis_with_gemini(analysis_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Gera um insight estrat√©gico em linguagem natural a partir dos resultados combinados.
    """
    if client is None:
        # Altera o status para 'FAIL' para que o analyst.py n√£o use o resultado
        print("ERRO CR√çTICO: summarize_analysis_with_gemini - Cliente Gemini n√£o inicializado.")
        return {'status': 'FAIL', 'error': 'Cliente Gemini n√£o inicializado.'} 
    
    topic_insights = analysis_data.get('topics_data', [])
    
    formatted_topics = ""
    for item in topic_insights:
        formatted_topics += f"- T√≥pico: {item.get('nome', 'N/A')} (Contagem: {item.get('contagem', 0)})\n"
        
    prompt = f"""
    Voc√™ √© um Consultor Estrat√©gico de Qualidade.
    Com base nos dados fornecidos da an√°lise de t√≥picos, forne√ßa um √∫nico par√°grafo de 3 a 4 frases de Insight Estrat√©gico.

    Foco do Insight:
    1. CONECTE a causa raiz mais frequente com a necessidade de a√ß√£o imediata.
    2. SUGIRA o departamento ou processo que deve ser auditado.
    3. CRIE um tom de urg√™ncia e clareza.

    Dados da An√°lise de T√≥picos (NLP):
    ---
    {formatted_topics}
    ---
    
    Insight Estrat√©gico:
    """
    
    try:
        # PONTO CHAVE: Usando run_in_threadpool
        response = await run_in_threadpool(
            _generate_content_sync,
            GEMINI_MODEL,
            prompt
        )

        return {
            'status': 'OK',
            'strategic_insight': response.text.strip()
        }
    
    except APIError as e:
        # Log detalhado para o debug da falha 'An√°lise Avan√ßada da IA'
        print(f"ERRO API (Summarize): Falha ao gerar insight estrat√©gico. Detalhe: {e}")
        return {'status': 'FAIL', 'error': f'Erro na API Gemini durante a sumariza√ß√£o. Verifique a chave e cota de uso: {e}'}
    except Exception as e:
        # Log detalhado para o debug do erro gen√©rico
        print(f"ERRO GEN√âRICO (Summarize): Erro desconhecido na sumariza√ß√£o. Detalhe: {e}")
        return {'status': 'FAIL', 'error': f'Erro desconhecido na sumariza√ß√£o: {e}'}